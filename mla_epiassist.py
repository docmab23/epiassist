# -*- coding: utf-8 -*-
"""MLA_EpiAssist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IcZHWEqhoPmPzY6hUvgdAnxQYbFBh4IZ
"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support, f1_score
from sklearn.metrics import confusion_matrix, precision_recall_curve
def generate_datasets_for_training(data, window_size,scale=True, scaler_type=StandardScaler):
  _l = len(data) 
  data = scaler_type().fit_transform(data)
  Xs = []
  Ys = []
  for i in range(0, (_l - window_size)):
    # because this is an autoencoder - our Ys are the same as our Xs. No need to pull the next sequence of values
    Xs.append(data[i:i+window_size])
    Ys.append(data[i:i+window_size])
  tr_x, ts_x, tr_y, ts_y = [np.array(x) for x in train_test_split(Xs, Ys)]
  assert tr_x.shape[2] == ts_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))
  return  (tr_x.shape[2], tr_x, tr_y, ts_x, ts_y)

!pip install ann_visualizer

import pandas as pd
data = pd.read_csv("halo.csv")
data =data.drop(["Unnamed: 0"],1)
data

feats, X, XX, Y, YY = generate_datasets_for_training(data,30)
XX.shape

Y.shape

epochs = 100
batch_size = 32
window_length = 30
from keras import metrics
import keras
import tensorflow as tf
import os

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=1e-2, patience=5, verbose=0, mode='auto',
    baseline=None, restore_best_weights=True)
feats,X,XX,Y,YY = generate_datasets_for_training(data,window_length)
#feats, X, Y, XX, YY = generate_datasets_for_training(data,window_length)

model = keras.Sequential()
model.add(keras.layers.LSTM(128, kernel_initializer='he_uniform', batch_input_shape=(None, window_length, feats), return_sequences=True, name='encoder_1'))
model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', batch_input_shape=(None, window_length, feats), return_sequences=True, name='encoder_2'))
model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='encoder_3'))
model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=True, name='encoder_4'))
model.add(keras.layers.LSTM(8, kernel_initializer='he_uniform', return_sequences=False, name='encoder_5'))
model.add(keras.layers.RepeatVector(window_length, name='encoder_decoder_bridge'))
model.add(keras.layers.LSTM(8, kernel_initializer='he_uniform',return_sequences=True, name='decoder_1'))
model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=True, name='decoder_2'))
model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='decoder_3'))
model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', return_sequences=True, name='decoder_4'))
model.add(keras.layers.LSTM(128, kernel_initializer='he_uniform', return_sequences=True, name='decoder_5'))

model.add(keras.layers.TimeDistributed(keras.layers.Dense(feats)))
model.compile(loss="mse",optimizer='adam')
model.build()
print(model.summary())

model.fit(x=X, y=X,epochs=100,validation_data=(Y, Y) ,batch_size=batch_size, shuffle=True, callbacks=[early_stop])
#feats, X, Y, XX, YY

model.save("Epiassist_model.h5")

from ann_visualizer.visualize import ann_viz
ann_viz(model, view=True, filename="MLA_epiassist.gv")

def flatten(tensor):
    '''
    Flatten a 3D array.
    
    Input
    X            A 3D array for lstm, where the array is sample x timesteps x features.
    
    Output
    flattened_X  A 2D array, sample x features.
    '''
    flattened_X = np.empty((tensor.shape[0], tensor.shape[2]))  # sample x features array.
    for i in range(tensor.shape[0]):
        flattened_X[i] = tensor[i, (tensor.shape[1]-1), :]
    return(flattened_X)

np.maneflatten(Y).shape

valid_x_predictions = model.predict(X)
valid_test_prediction = model.predict(Y)
from sklearn.metrics import mean_squared_error
#mse = np.sqrt(np.mean(np.power(flatten(X) - flatten(valid_x_predictions), 2), axis=1))
#mse2 = np.sqrt(np.mean(np.power(flatten(Y) - flatten(valid_test_prediction), 2), axis=1))
#plt.plot(mse)
#np.max(mse)
#len(np.corrcoef(flatten(X),flatten(valid_x_predictions)))
#error_df = pd.DataFrame({'Reconstruction_error': mse,
                        #'True_class': np.mean(flatten(Y))})
#error_df.describe()
#print(np.max(mse) , np.max(mse2))
rmse1 =np.sqrt(mean_squared_error(flatten(X),flatten(valid_x_predictions)))
rmse2 = (mean_squared_error(flatten(Y) , flatten(valid_test_prediction)))
print(f"insample score:{rmse1}" , f"outsample score: {rmse2}")

'''rx =[]
for i in range((X).shape[0]): 
   r = np.sqrt(mean_squared_error(flatten(X)[i] , flatten(valid_x_predictions)[i]))
   rx.append(r)'''

#plt.hist(rx,bins=50)

loss= tf.keras.losses.mae(flatten(X),flatten(valid_x_predictions))
loss2 = tf.keras.losses.mae(flatten(Y),flatten(valid_test_prediction))
plt.hist(loss,bins=50)
plt.xlabel("Reconstruction error")
plt.ylabel("no of samples")
mean = np.mean(loss)
std = np.std(loss)
print(mean, std)
threshold = mean + 3*std 
Threshold = "Threshold"
plt.axvline(threshold, color="r",linewidth=3,linestyle="dashed",label= "Threshold = 1.4595")
plt.axvline(mean,color="y",linestyle="dashed",label= "MRE(Mean Reconstruction Error")
plt.legend(loc="upper left")

f = open("threshold.txt", "a")
f.write(str(threshold))

test_mae_loss = np.mean(np.abs(valid_test_prediction - Y), axis=1)
test_mae_loss_avg_vector = np.mean(test_mae_loss, axis=1)
test_mae_loss_avg_vector

import numpy as np
flattened_actual =np.array(flatten(X))
flattened_predicts = flatten(valid_x_predictions)
rm = (flatten(Y-valid_test_prediction))
rm

dim_1 =flattened_actual[:,0]-flattened_predicts[:,0]

plt.plot(dim_1)
#flattened_actual.shape

from sklearn.metrics import confusion_matrix, precision_recall_curve
from sklearn.metrics import recall_score, classification_report, auc, roc_curve
from sklearn.metrics import precision_recall_fscore_support, f1_score
false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.Reconstruction_error, error_df.True_class)
roc_auc = auc(false_pos_rate, true_pos_rate,)

'''plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)
plt.plot([0,1],[0,1], linewidth=5)

plt.xlim([-0.01, 1])
plt.ylim([0, 1.01])
plt.legend(loc='lower right')
plt.title('Receiver operating characteristic curve (ROC)')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()'''

plt.plot(X[0])
plt.plot(valid_x_predictions[0])
plt.show()

#flatten(X).shape
#len(mse)
#hh =flatten(XX).mean(axis=0)
XX = (flatten(XX).mean(axis=1))

#valid_x_predictions = lstm_autoencoder.predict(X_valid_scaled)
#mse = np.mean(np.power(flatten(X_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)

error_df = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': XX})

precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)
plt.plot(threshold_rt, precision_rt[1:], label="Precision",linewidth=5)
plt.plot(threshold_rt, recall_rt[1:], label="Recall",linewidth=5)
plt.title('Precision and recall for different threshold values')
plt.xlabel('Threshold')
plt.ylabel('Precision/Recall')
plt.legend()
plt.show()
mse

def split_series(series, n_past, n_future):
  #
  # n_past ==> no of past observations
  #
  # n_future ==> no of future observations 
  #
  X, y = list(), list()
  for window_start in range(len(series)):
    past_end = window_start + n_past
    future_end = past_end + n_future
    if future_end > len(series):
      break
    # slicing the past and future parts of the window
    past, future = series[window_start:past_end, :], series[past_end:future_end, :]
    X.append(past)
    y.append(future)
  return np.array(X), np.array(y)
split_series(data , 30 , 1)